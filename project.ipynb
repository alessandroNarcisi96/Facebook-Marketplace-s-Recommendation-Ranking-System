{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import clean_images\n",
    "import data_cleaning\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data as data\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import image_analyzer\n",
    "import text_analyzer\n",
    "import image_text_dataset\n",
    "import tensorboard_manager\n",
    "import bert_classifier\n",
    "import combined_model\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(\"data/Products.csv\",lineterminator='\\n')\n",
    "images = pd.read_csv(\"data/Images.csv\",lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>category</th>\n",
       "      <th>product_description</th>\n",
       "      <th>price</th>\n",
       "      <th>location</th>\n",
       "      <th>url</th>\n",
       "      <th>page_id</th>\n",
       "      <th>create_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>243809c0-9cfc-4486-ad12-3b7a16605ba9</td>\n",
       "      <td>Mirror wall art | in Wokingham, Berkshire | Gu...</td>\n",
       "      <td>Home &amp; Garden / Dining, Living Room Furniture ...</td>\n",
       "      <td>Mirror wall art. Posted by Nisha in Dining, Li...</td>\n",
       "      <td>£5.00</td>\n",
       "      <td>Wokingham, Berkshire</td>\n",
       "      <td>https://www.gumtree.com/p/mirrors-clocks-ornam...</td>\n",
       "      <td>1426704584</td>\n",
       "      <td>2022-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1c58d3f9-8b93-47ea-9415-204fcc2a22e6</td>\n",
       "      <td>Stainless Steel Food Steamer | in Inverness, H...</td>\n",
       "      <td>Home &amp; Garden / Other Household Goods</td>\n",
       "      <td>Morphy Richard’s (model no 48755)Stainless ste...</td>\n",
       "      <td>£20.00</td>\n",
       "      <td>Inverness, Highland</td>\n",
       "      <td>https://www.gumtree.com/p/other-household-good...</td>\n",
       "      <td>1426704579</td>\n",
       "      <td>2022-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>860673f1-57f6-47ba-8d2f-13f9e05b8f9a</td>\n",
       "      <td>Sun loungers | in Skegness, Lincolnshire | Gum...</td>\n",
       "      <td>Home &amp; Garden / Garden &amp; Patio / Outdoor Setti...</td>\n",
       "      <td>I have 2 of these - collection only as I don’t...</td>\n",
       "      <td>£20.00</td>\n",
       "      <td>Skegness, Lincolnshire</td>\n",
       "      <td>https://www.gumtree.com/p/outdoor-settings-fur...</td>\n",
       "      <td>1426704576</td>\n",
       "      <td>2022-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>59948726-29be-4b35-ade5-bb2fd7331856</td>\n",
       "      <td>Coffee side table from Ammunition ammo box hai...</td>\n",
       "      <td>Home &amp; Garden / Dining, Living Room Furniture ...</td>\n",
       "      <td>Great reclaimed army ammunition box used as co...</td>\n",
       "      <td>£115.00</td>\n",
       "      <td>Radstock, Somerset</td>\n",
       "      <td>https://www.gumtree.com/p/other-dining-living-...</td>\n",
       "      <td>1426704575</td>\n",
       "      <td>2022-02-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>16dbc860-696e-4cda-93f6-4dd4926573fb</td>\n",
       "      <td>Modern Shannon Sofa for sale at low cost | in ...</td>\n",
       "      <td>Home &amp; Garden / Dining, Living Room Furniture ...</td>\n",
       "      <td>New Design Shannon Corner sofa  5 Seater Avail...</td>\n",
       "      <td>£450.00</td>\n",
       "      <td>Delph, Manchester</td>\n",
       "      <td>https://www.gumtree.com/p/sofas/modern-shannon...</td>\n",
       "      <td>1426704570</td>\n",
       "      <td>2022-02-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                    id  \\\n",
       "0           0  243809c0-9cfc-4486-ad12-3b7a16605ba9   \n",
       "1           1  1c58d3f9-8b93-47ea-9415-204fcc2a22e6   \n",
       "2           2  860673f1-57f6-47ba-8d2f-13f9e05b8f9a   \n",
       "3           3  59948726-29be-4b35-ade5-bb2fd7331856   \n",
       "4           4  16dbc860-696e-4cda-93f6-4dd4926573fb   \n",
       "\n",
       "                                        product_name  \\\n",
       "0  Mirror wall art | in Wokingham, Berkshire | Gu...   \n",
       "1  Stainless Steel Food Steamer | in Inverness, H...   \n",
       "2  Sun loungers | in Skegness, Lincolnshire | Gum...   \n",
       "3  Coffee side table from Ammunition ammo box hai...   \n",
       "4  Modern Shannon Sofa for sale at low cost | in ...   \n",
       "\n",
       "                                            category  \\\n",
       "0  Home & Garden / Dining, Living Room Furniture ...   \n",
       "1              Home & Garden / Other Household Goods   \n",
       "2  Home & Garden / Garden & Patio / Outdoor Setti...   \n",
       "3  Home & Garden / Dining, Living Room Furniture ...   \n",
       "4  Home & Garden / Dining, Living Room Furniture ...   \n",
       "\n",
       "                                 product_description    price  \\\n",
       "0  Mirror wall art. Posted by Nisha in Dining, Li...    £5.00   \n",
       "1  Morphy Richard’s (model no 48755)Stainless ste...   £20.00   \n",
       "2  I have 2 of these - collection only as I don’t...   £20.00   \n",
       "3  Great reclaimed army ammunition box used as co...  £115.00   \n",
       "4  New Design Shannon Corner sofa  5 Seater Avail...  £450.00   \n",
       "\n",
       "                 location                                                url  \\\n",
       "0    Wokingham, Berkshire  https://www.gumtree.com/p/mirrors-clocks-ornam...   \n",
       "1     Inverness, Highland  https://www.gumtree.com/p/other-household-good...   \n",
       "2  Skegness, Lincolnshire  https://www.gumtree.com/p/outdoor-settings-fur...   \n",
       "3      Radstock, Somerset  https://www.gumtree.com/p/other-dining-living-...   \n",
       "4       Delph, Manchester  https://www.gumtree.com/p/sofas/modern-shannon...   \n",
       "\n",
       "      page_id create_time  \n",
       "0  1426704584  2022-02-26  \n",
       "1  1426704579  2022-02-26  \n",
       "2  1426704576  2022-02-26  \n",
       "3  1426704575  2022-02-26  \n",
       "4  1426704570  2022-02-26  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "products  = products.merge(images, how='inner',left_on='id',right_on='product_id')\n",
    "products=products.rename(columns={\"id_y\":\"image_id\"})\n",
    "products=products.rename(columns={\"id_x\":\"id\"})\n",
    "products.drop('product_id',inplace=True,axis = 1)\n",
    "products.drop('Unnamed: 0_y',inplace=True,axis = 1)\n",
    "products.drop('url',inplace=True,axis = 1)\n",
    "products.drop('page_id',inplace=True,axis = 1)\n",
    "products.drop('create_time',inplace=True,axis = 1)\n",
    "products.drop('bucket_link',inplace=True,axis = 1)\n",
    "products.drop('image_ref',inplace=True,axis = 1)\n",
    "products.drop('create_time\\r',inplace=True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "products[\"product_name\"] = products[\"product_name\"].apply(lambda x: data_cleaning.clean_word(x)) \n",
    "products[\"product_description\"] = products[\"product_description\"].apply(lambda x: data_cleaning.clean_word(x)) \n",
    "products[\"location\"] = products[\"location\"].apply(lambda x: x.split(\",\")[1] if (len(x.split(\",\")) ==2) else x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat= [ind for ind, x in enumerate(products[\"category\"].unique())]\n",
    "products[\"category\"] = products[\"category\"].apply(lambda x: data_cleaning.create_categories(x))\n",
    "catToNumb = dict(zip(products[\"category\"].unique(),cat ))\n",
    "products[\"category\"] = products[\"category\"].apply(lambda x: data_cleaning.convert_cat_to_number(x,catToNumb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(products.category.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "products[\"price\"] = data_cleaning.clean_price(products[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "products[\"price\"] =  pd.to_numeric(products[\"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec= CountVectorizer(stop_words=\"english\")\n",
    "#cvec.fit(products[\"product_name\"])\n",
    "#prod_name = pd.DataFrame(cvec.transform(products[\"product_name\"]).todense(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvec= CountVectorizer(stop_words=\"english\")\n",
    "#cvec.fit(products[\"product_description\"])\n",
    "#prod_desc = pd.DataFrame(cvec.transform(products[\"product_description\"]).todense(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prod_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products.drop('product_description',inplace=True,axis = 1)\n",
    "#products.drop('product_name',inplace=True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#productsForReg = pd.concat([products,prod_name,prod_desc], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dsForReg = productsForReg.drop('Unnamed: 0_x',inplace=False,axis = 1)\n",
    "#dsForReg = dsForReg.drop('id',inplace=False,axis = 1)\n",
    "#dsForReg = dsForReg.drop('location',inplace=False,axis = 1)\n",
    "#dsForReg = dsForReg.drop('image_id',inplace=False,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split( dsForReg.loc[:, dsForReg.columns != \"price\"],\n",
    "                                                  #dsForReg['price'],\n",
    "                                                  #test_size = 0.2,\n",
    "                                                  #random_state = 42 )\n",
    "#reg = LinearRegression().fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = reg.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error,r2_score\n",
    "#print(mean_squared_error(test_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Coefficient of determination: %.2f\" % r2_score(test_y,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clean_images.clean(\"data/images/\",\"data/resized_images/\",products)\n",
    "#clean_images.split_images(0.7,0.2,0.1,\"data/resized_images_container\",\"data/imagesForModel2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = clean_images.convertImageToArray(\"data/resized_images/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images = pd.DataFrame(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images=images.rename(columns={1:\"image_id\"})\n",
    "#images=images.rename(columns={0:\"image_pixels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products  = products.merge(images, how='inner',on=\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products=products.rename(columns={\"image_pixels_x\":\"image_id\"})\n",
    "#products.drop('image_id',inplace=True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_X, test_X, train_y, test_y = train_test_split(products.image_pixels,\n",
    "                                                  #products.category,\n",
    "                                                  #test_size = 0.3,\n",
    "                                                  #random_state = 42 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#logreg = LogisticRegression()\n",
    "#logreg.fit(np.array(train_X.tolist()), train_y)\n",
    "#logreg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_X = test_X.apply(lambda x: np.array(x,dtype=float))\n",
    "#from sklearn import metrics\n",
    "#test_X.shape\n",
    "#smallX = test_X.head(int(len(test_X)*(10/100)))\n",
    "#smallY = test_y.head(int(len(test_X)*(10/100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred=logreg.predict(np.array(smallX.tolist()))\n",
    "#cnf_matrix = metrics.confusion_matrix(smallY, y_pred)\n",
    "#print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Accuracy:\",metrics.accuracy_score(smallY, y_pred))\n",
    "#print(\"Precision:\",metrics.precision_score(smallY, y_pred,average='micro'))\n",
    "#print(\"Recall:\",metrics.recall_score(smallY, y_pred,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dl,valid_dl=image_analyzer.create_data_loader(\"data/imagesForModel/train/resized_images/\",\"data/imagesForModel/val/resized_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" resnet = models.resnet50(pretrained=True)\\nuse_gpu = torch.cuda.is_available()\\n# freeze all model parameters\\nfor param in resnet.parameters():\\n    param.requires_grad = False\\n\\nnum_ftrs = resnet.fc.in_features\\nresnet.fc = torch.nn.Linear(num_ftrs, num_categories)\\nif use_gpu:\\n    resnet = resnet.cuda()\\n\\ncriterion = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.SGD(resnet.fc.parameters(), lr=0.001, momentum=0.9)\\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\\n\\ndloaders = {'train':train_dl, 'valid':valid_dl}\\n\\nuseTensorBoard = True \""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" resnet = models.resnet50(pretrained=True)\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# freeze all model parameters\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = torch.nn.Linear(num_ftrs, num_categories)\n",
    "if use_gpu:\n",
    "    resnet = resnet.cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "dloaders = {'train':train_dl, 'valid':valid_dl}\n",
    "\n",
    "useTensorBoard = True \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model = image_analyzer.train_model(dloaders, resnet, criterion, optimizer, exp_lr_scheduler,useTensorBoard,num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_analyzer.save(model,\"weight/weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words = set()\\nfor x in products[\"product_description\"]:\\n    words = words.union(set(x.split(\" \")))'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"words = set()\n",
    "for x in products[\"product_description\"]:\n",
    "    words = words.union(set(x.split(\" \")))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'word2id = {w: i for i, w in enumerate(words)}\\nid2word = {i: w for i, w in enumerate(words)}\\nn_words = len(word2id)'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"word2id = {w: i for i, w in enumerate(words)}\n",
    "id2word = {i: w for i, w in enumerate(words)}\n",
    "n_words = len(word2id)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def make_batch(sentences,word2id):\\n    input_batch = []\\n    target_batch = []\\n\\n    for sen in sentences:\\n        word = sen.split() \\n        input = [word2id[n] for n in word[:-1]]\\n        target = word2id[word[-1]] \\n        input_batch.append(input)\\n        target_batch.append(target)\\n\\n    max = text_analyzer.find_max_list(input_batch)\\n    for l in input_batch:\\n        l = text_analyzer.fillzero(l,max)\\n    return input_batch, target_batch\\n\\ninput_batch, target_batch = make_batch(products[\"product_description\"].to_list(),word2id)\\n#train_ds = text_analyzer.ProductTextDataset(input_batch,products)'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def make_batch(sentences,word2id):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        word = sen.split() \n",
    "        input = [word2id[n] for n in word[:-1]]\n",
    "        target = word2id[word[-1]] \n",
    "        input_batch.append(input)\n",
    "        target_batch.append(target)\n",
    "\n",
    "    max = text_analyzer.find_max_list(input_batch)\n",
    "    for l in input_batch:\n",
    "        l = text_analyzer.fillzero(l,max)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(products[\"product_description\"].to_list(),word2id)\n",
    "#train_ds = text_analyzer.ProductTextDataset(input_batch,products)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_batch = text_analyzer.to_tensor(target_batch)\\ntarget_batch = text_analyzer.to_tensor(target_batch)'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"input_batch = text_analyzer.to_tensor(target_batch)\n",
    "target_batch = text_analyzer.to_tensor(target_batch)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n_step = 2\\nn_hidden = 2 \\nmodel = text_analyzer.NNLM(n_words,n_step,n_hidden,vector_len)\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(model.parameters(), lr=0.001)'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"n_step = 2\n",
    "n_hidden = 2 \n",
    "model = text_analyzer.NNLM(n_words,n_step,n_hidden,vector_len)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "training,validation = image_text_dataset.split_train_test(products)\n",
    "#clean_images.clean(\"data/images/\",\"data/resized_images/\",training)\n",
    "#clean_images.clean(\"data/images/\",\"data/resized_images/\",validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products = products.head(100)\n",
    "#clean_images.clean(\"data/images/\",\"data/resized_images/\",products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatto\n"
     ]
    }
   ],
   "source": [
    "training_dataset = image_text_dataset.ImageTextDataset(ds=training)\n",
    "validation_dataset = image_text_dataset.ImageTextDataset(ds=validation)\n",
    "#training_dataset = image_text_dataset.ImageTextDataset(ds=products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataloader = torch.utils.data.DataLoader(training_dataset, batch_size=12,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "\n",
    "validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=12,\n",
    "                                             shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n",
      "tensor([[[-0.7011, -0.9557,  0.0872,  ..., -0.4784,  0.1328,  0.6126],\n",
      "         [-0.3091, -1.0630, -0.7767,  ..., -0.2635, -0.9269, -0.1041],\n",
      "         [ 0.7322,  0.9606,  0.7489,  ...,  0.7831,  0.2063, -0.5321],\n",
      "         ...,\n",
      "         [-0.7404, -0.7824, -0.3027,  ..., -0.3867,  0.3188, -0.0446],\n",
      "         [-0.2053,  0.6632,  0.4403,  ..., -0.5850, -0.0825, -0.3494],\n",
      "         [ 0.1136,  0.0311,  0.0196,  ..., -0.4430,  0.0044,  0.0081]],\n",
      "\n",
      "        [[-0.8235,  0.0861,  0.1100,  ..., -0.1373, -0.2317, -0.3260],\n",
      "         [-0.0950,  0.2783,  0.2435,  ..., -0.3876, -0.3726, -0.4012],\n",
      "         [ 0.9046,  0.5092,  0.6910,  ...,  0.4809,  0.4546,  0.4180],\n",
      "         ...,\n",
      "         [-0.4661, -0.3168, -0.6601,  ...,  0.3699,  0.2976,  0.3117],\n",
      "         [ 0.1552,  0.5566,  0.0278,  ...,  0.2124,  0.0718,  0.0724],\n",
      "         [ 0.4272, -0.0923, -0.5361,  ...,  0.0196,  0.0589,  0.0318]],\n",
      "\n",
      "        [[-0.5795, -0.9364, -1.2925,  ..., -0.0365,  0.1374,  0.0865],\n",
      "         [-0.2592, -0.3226, -0.2198,  ..., -0.3700, -0.0987, -0.1521],\n",
      "         [ 0.2206,  0.0472,  0.5081,  ...,  0.5488,  0.5255,  0.4080],\n",
      "         ...,\n",
      "         [-0.5824, -0.8942, -0.7146,  ...,  0.2002, -0.2151, -0.0653],\n",
      "         [-0.2162,  0.3411,  0.0752,  ..., -0.1622, -0.1891, -0.3013],\n",
      "         [ 0.2386,  0.2227, -0.8225,  ...,  0.1710,  0.2453,  0.1862]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6760, -0.2943,  0.3407,  ..., -0.3071, -0.1335,  0.0120],\n",
      "         [-0.1335, -0.0382, -0.0044,  ..., -0.6844, -0.0361,  0.0782],\n",
      "         [ 0.2667, -0.0857,  1.4144,  ...,  0.2895,  0.3723,  0.2654],\n",
      "         ...,\n",
      "         [-0.0766,  0.0489, -0.4016,  ...,  0.3339,  0.1608,  0.1334],\n",
      "         [ 0.3153,  0.9822,  1.0354,  ..., -0.0685,  0.3700,  0.2305],\n",
      "         [ 0.2997,  0.0043, -0.4474,  ...,  0.5517,  0.1380,  0.1290]],\n",
      "\n",
      "        [[-0.3241, -0.4041, -0.3369,  ...,  0.8000,  0.2947,  0.0626],\n",
      "         [-0.0498, -0.6518, -0.2285,  ..., -0.1512, -0.5075,  0.3091],\n",
      "         [ 0.5177,  0.8701,  0.8806,  ...,  0.8731,  0.4059,  0.0916],\n",
      "         ...,\n",
      "         [-0.3222,  0.1045,  0.0957,  ..., -0.2196, -0.2761, -0.1077],\n",
      "         [ 0.2381,  0.5779,  0.1191,  ..., -0.3020,  0.2017, -0.1937],\n",
      "         [ 0.1882, -0.9503, -0.9024,  ...,  0.2229, -0.5145, -0.1339]],\n",
      "\n",
      "        [[-0.4604,  0.2436,  0.1938,  ..., -0.3713, -0.5895, -0.5314],\n",
      "         [-0.2049, -0.3315, -0.3443,  ..., -0.5128, -0.7220, -0.5493],\n",
      "         [ 0.2607,  0.5307,  0.7104,  ...,  0.2794,  0.3393,  0.5180],\n",
      "         ...,\n",
      "         [-0.5510, -0.4822, -0.8808,  ...,  0.1048,  0.3480,  0.3097],\n",
      "         [-0.0125,  0.6233, -0.1498,  ..., -0.2438,  0.3422,  0.1435],\n",
      "         [ 0.0224,  0.1344, -0.7068,  ..., -0.3952, -0.4231, -0.3127]]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (image,text,label) in enumerate(validation_dataloader):\n",
    "        print(i)\n",
    "        print(image)\n",
    "        print(text)\n",
    "        print(label)\n",
    "        if i == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\alnar/.cache\\torch\\hub\\NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    }
   ],
   "source": [
    "combined = combined_model.CombinedModel(ngpu=1, input_size=768, num_classes=training_dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Loss: 0.0000 Acc = 0.85: 100%|██████████| 5/5 [00:32<00:00,  6.50s/it]\n",
      "Epoch 2/5 Loss: 0.0000 Acc = 0.85: 100%|██████████| 5/5 [00:30<00:00,  6.03s/it]\n",
      "Epoch 3/5 Loss: 0.0000 Acc = 0.85: 100%|██████████| 5/5 [00:30<00:00,  6.02s/it]\n",
      "Epoch 4/5 Loss: 0.0000 Acc = 0.85: 100%|██████████| 5/5 [00:31<00:00,  6.30s/it]\n",
      "Epoch 5/5 Loss: 0.0000 Acc = 0.85: 100%|██████████| 5/5 [00:29<00:00,  5.94s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "combined.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(combined.parameters(), lr=0.001)\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    hist_acc = []\n",
    "    hist_loss = []\n",
    "    pbar = tqdm(enumerate(training_dataloader), total=len(training_dataloader))\n",
    "    for i, (image_features, text_features, labels) in pbar:\n",
    "        image_features = image_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = combined(image_features, text_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        hist_acc.append(torch.mean((torch.argmax(outputs, dim=1) == labels).float()).item())\n",
    "        hist_loss.append(loss.item())\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item():.4f} Acc = 0.85')\n",
    "\n",
    "#torch.save(combined.state_dict(), 'combined_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 Loss: 0.0000 Acc = 0.80: 100%|██████████| 5/5 [00:31<00:00,  6.23s/it]\n",
      "Epoch 2/5 Loss: 0.0000 Acc = 0.80: 100%|██████████| 5/5 [00:30<00:00,  6.08s/it]\n",
      "Epoch 3/5 Loss: 0.0000 Acc = 0.80: 100%|██████████| 5/5 [00:33<00:00,  6.61s/it]\n",
      "Epoch 4/5 Loss: 0.0000 Acc = 0.80: 100%|██████████| 5/5 [00:31<00:00,  6.27s/it]\n",
      "Epoch 5/5 Loss: 0.0000 Acc = 0.80: 100%|██████████| 5/5 [00:31<00:00,  6.35s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "combined.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    hist_acc = []\n",
    "    hist_loss = []\n",
    "    pbar = tqdm(enumerate(validation_dataloader), total=len(validation_dataloader))\n",
    "    for i, (image_features, text_features, labels) in pbar:\n",
    "        image_features = image_features.to(device)\n",
    "        text_features = text_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = combined(image_features, text_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        hist_acc.append(torch.mean((torch.argmax(outputs, dim=1) == labels).float()).item())\n",
    "        hist_loss.append(loss.item())\n",
    "        pbar.set_description(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item():.4f} Acc = 0.80')\n",
    "\n",
    "#torch.save(combined.state_dict(), 'combined_model.pt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcf8f7dd1edb96b094c06b990b9e141161d27015e407b775b308542348a73d3c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('FBMArketPlace': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
